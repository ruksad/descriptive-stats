{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following statements the first time\n",
    "# if you do not have the libraries installed\n",
    "\n",
    "#!pip install nltk\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6feb7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9f03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "file_name = '../../source/Session-Summary-sample.xlsx'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_excel(file_name)\n",
    "df_work = df_orig.copy()\n",
    "\n",
    "# Convert RollNo to uppercase and Session_Summary to lowercase\n",
    "df_work['RollNo'] = df_work['RollNo'].astype(str).str.upper()\n",
    "df_work['Session_Summary'] = df_work['Session_Summary'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4184ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58 entries, 0 to 57\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Timestamp         58 non-null     object \n",
      " 1   Username          58 non-null     object \n",
      " 2   RollNo            58 non-null     object \n",
      " 3   Session_Summary   58 non-null     object \n",
      " 4   Questions         0 non-null      float64\n",
      " 5   Comments          2 non-null      object \n",
      " 6   Total_Characters  58 non-null     int64  \n",
      " 7   Total_Words       58 non-null     int64  \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 3.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Add columns for total number of characters and words in Session_Summary\n",
    "df_work['Total_Characters'] = df_work['Session_Summary'].str.len()\n",
    "df_work['Total_Words'] = df_work['Session_Summary'].str.split().apply(len)\n",
    "df_work.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb98fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c7a937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Username",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RollNo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Session_Summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Questions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Comments",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Total_Characters",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Words",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "aad442c1-b810-458d-83b8-95d5167cc487",
       "rows": [
        [
         "0",
         "2025/01/22 10:57:42 AM GMT+5:30",
         "u01",
         "U01",
         "the class started with a hands on demo. sir uploaded a file on moodle. we then downloaded the file and applied simple linear regression on it. sir then explained about some features in excel and how to use them. sir then focused on errors in linear regression. then sir explained about histogram. outcome dependent on a large number of unknown causes(random), the distribution is gaussian normal distribution.\nwe need to understand what each number we get from analysis tells us. lower and upper 95% means that the real value of population lies with 95% confidence in the interval [lower 95,upper 95].\nwhat is a good model- one that explains most of the variations in the data. \nsst=sum of (yi-y_bar)â² (measure of total variation in given dataset)\nsst=sse+ssr\nssr- sum of square of regression line (total variation explained by the regression line)\nsse- variation not explained by the model, attributed to random errors. \nsst=ssr+sse\n1=(ssr/sst)+sse/sst\nssr/sst portion of total variation described by the regression model. for a nice model we need this value to be as high as possible. this value is called r squared value. lies in between 0 and 1.  this is known as coefficient of determination. for the case of simple linear regression, the coefficient of determination is same as square of the correlation coefficient r between x and y. so this is called 'r square'. correlation coefficient of x and y=cov(xy)/root(var(x)*var(y))\nr square should be ideally be close to 1. \nthen sir explained theory behind interval estimates and point estimates. ",
         null,
         null,
         "1551",
         "253"
        ],
        [
         "1",
         "2025/01/22 10:58:35 AM GMT+5:30",
         "u02",
         "U02",
         "learnt analysing data in excel and different new terms of data analysis",
         null,
         null,
         "71",
         "12"
        ],
        [
         "2",
         "2025/01/22 11:07:54 AM GMT+5:30",
         "u03",
         "U03",
         "in todayâ€™s hands-on class, we worked with a dataset to perform regression analysis. using excel, we calculated key values like x_bar (average of x) and y_bar (average of y) manually using formulas. we explored the concept of error and its gaussian (normal) distribution, and understood the sum of squares of residuals (ssr), total sum of squares (sst), and explained sum of squares (sse). we discussed how well the variance is captured by the regression line and learned that, for simple linear regression, the coefficient of determination (r square) is the square of the correlation coefficient (r). finally, we used the data analysis toolpak in excel to quickly generate regression statistics for the dataset. additionally, we discussed sampling concepts, including calculating sample means from multiple samples, finding the mean of these sample means, and determining their variance and standard deviation. we understood that there is some relationship between the variance of sample means and the variance of the original data.",
         null,
         null,
         "1034",
         "159"
        ],
        [
         "3",
         "2025/01/22 11:10:12 AM GMT+5:30",
         "u04",
         "U04",
         "we first compared about the estimators being the statistics of the population. later we used excel to perform linear reg with the help of the formula provided in prev and class and performing the linear reg with the help of xlminer toolpack. later we talked about the ei errors in the regression. commenting on the error whether it is random or uniform. usually when the model explains the data variance successfully the errors should be close to a random. how should we comment on randomly distributed errors of regression? it should follow a gaussian distribution. then we can say our model knows all the parameters which can be the reason for the variation of the data. there were several results statistics like r^2 anova which given when we used xlminer. so like r^2 shows how much variation of the data does the particular model explains. we later moved on the proof of r^2 which is derived from sst,ssr,sse. r^2 was ssr/sst. r^2 is also called coef of determination. but why square? for the case of slr, the coeff of determination is the sq(correlation between x&y). since correlation coeff=> r. we call coeff of determination as r^2 (but only for slr(simple linear regression)). what is correlation coef. : as x changes wrt to its mean. how y changes wrt its mean. positive and negative correlation coeff shows how y changes wrt x (positively or negatively.) higher the value of r^2 higher explanation the model gives on the data variance. later at last we touched upon the standard error.",
         null,
         null,
         "1497",
         "257"
        ],
        [
         "4",
         "2025/01/22 11:25:40 AM GMT+5:30",
         "u05",
         "U05",
         "when predicting with a given sample size using linear regression (or any regression), there are limits. we can make predictions, but only up to a certain point , not far beyond the sample.\n\ntime series analysis: time series analysis helps to predict beyond the sample size. \n\nunderstanding f(x): in the equation ð‘“(ð‘¥)=ð‘¦ , the trend of the data is the signal, while the deviations are the noise. if there is a trend in the noise, it means your model has not captured the trend perfectly.\n\nhistogram: a histogram is simply a frequency chart that shows how often different values appear in a dataset.\n\na good model: a good model is one that explains most of the variation in the data.\n\nin simple linear regression (slr):\nsst=sse+ssr\nwhere:\nsse: sum of squares of errors, or the variation not explained by the regression line.\nssr: sum of squares of regression, or the total variation explained by the regression line.\n\nr-squared (râ²):\nrâ² = ssr/sst = 1 - sse/sst \nrâ² is the coefficient of determination, which measures how well the regression model explains the variation in the data.\n\nwhy râ² and not r? in simple linear regression (slr), the coefficient of determination (râ²) is exactly the same as the square of the correlation coefficient between ð‘¥ and ð‘¦ this is why we use râ²  rather than ð‘….\n\nwe also created an excel program for linear regression and learned about some of the major terms used to evaluate how well the model explains the variation in the data.",
         null,
         null,
         "1484",
         "254"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>RollNo</th>\n",
       "      <th>Session_Summary</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Total_Characters</th>\n",
       "      <th>Total_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025/01/22 10:57:42 AM GMT+5:30</td>\n",
       "      <td>u01</td>\n",
       "      <td>U01</td>\n",
       "      <td>the class started with a hands on demo. sir up...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1551</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025/01/22 10:58:35 AM GMT+5:30</td>\n",
       "      <td>u02</td>\n",
       "      <td>U02</td>\n",
       "      <td>learnt analysing data in excel and different n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025/01/22 11:07:54 AM GMT+5:30</td>\n",
       "      <td>u03</td>\n",
       "      <td>U03</td>\n",
       "      <td>in todayâ€™s hands-on class, we worked with a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1034</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025/01/22 11:10:12 AM GMT+5:30</td>\n",
       "      <td>u04</td>\n",
       "      <td>U04</td>\n",
       "      <td>we first compared about the estimators being t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025/01/22 11:25:40 AM GMT+5:30</td>\n",
       "      <td>u05</td>\n",
       "      <td>U05</td>\n",
       "      <td>when predicting with a given sample size using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1484</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp Username RollNo  \\\n",
       "0  2025/01/22 10:57:42 AM GMT+5:30      u01    U01   \n",
       "1  2025/01/22 10:58:35 AM GMT+5:30      u02    U02   \n",
       "2  2025/01/22 11:07:54 AM GMT+5:30      u03    U03   \n",
       "3  2025/01/22 11:10:12 AM GMT+5:30      u04    U04   \n",
       "4  2025/01/22 11:25:40 AM GMT+5:30      u05    U05   \n",
       "\n",
       "                                     Session_Summary  Questions Comments  \\\n",
       "0  the class started with a hands on demo. sir up...        NaN      NaN   \n",
       "1  learnt analysing data in excel and different n...        NaN      NaN   \n",
       "2  in todayâ€™s hands-on class, we worked with a ...        NaN      NaN   \n",
       "3  we first compared about the estimators being t...        NaN      NaN   \n",
       "4  when predicting with a given sample size using...        NaN      NaN   \n",
       "\n",
       "   Total_Characters  Total_Words  \n",
       "0              1551          253  \n",
       "1                71           12  \n",
       "2              1034          159  \n",
       "3              1497          257  \n",
       "4              1484          254  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40125ae-b3ac-4f83-8eec-cf7e32a450e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50a20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc52e8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df_work['PP'] = df_work['Session_Summary'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3c8e7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Username",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RollNo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Session_Summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Questions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Comments",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Total_Characters",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PP",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "247dbcf2-6d0a-487a-ae98-7bbb279990db",
       "rows": [
        [
         "0",
         "2025/01/22 10:57:42 AM GMT+5:30",
         "u01",
         "U01",
         "the class started with a hands on demo. sir uploaded a file on moodle. we then downloaded the file and applied simple linear regression on it. sir then explained about some features in excel and how to use them. sir then focused on errors in linear regression. then sir explained about histogram. outcome dependent on a large number of unknown causes(random), the distribution is gaussian normal distribution.\nwe need to understand what each number we get from analysis tells us. lower and upper 95% means that the real value of population lies with 95% confidence in the interval [lower 95,upper 95].\nwhat is a good model- one that explains most of the variations in the data. \nsst=sum of (yi-y_bar)â² (measure of total variation in given dataset)\nsst=sse+ssr\nssr- sum of square of regression line (total variation explained by the regression line)\nsse- variation not explained by the model, attributed to random errors. \nsst=ssr+sse\n1=(ssr/sst)+sse/sst\nssr/sst portion of total variation described by the regression model. for a nice model we need this value to be as high as possible. this value is called r squared value. lies in between 0 and 1.  this is known as coefficient of determination. for the case of simple linear regression, the coefficient of determination is same as square of the correlation coefficient r between x and y. so this is called 'r square'. correlation coefficient of x and y=cov(xy)/root(var(x)*var(y))\nr square should be ideally be close to 1. \nthen sir explained theory behind interval estimates and point estimates. ",
         null,
         null,
         "1551",
         "253",
         "class started hand demo sir uploaded file moodle downloaded file applied simple linear regression sir explained feature excel use sir focused error linear regression sir explained histogram outcome dependent large number unknown causesrandom distribution gaussian normal distribution need understand number get analysis tell u lower upper mean real value population lie confidence interval lower upper good model one explains variation data sstsum yiybar measure total variation given dataset sstssessr ssr sum square regression line total variation explained regression line sse variation explained model attributed random error sstssrsse ssrsstssesst ssrsst portion total variation described regression model nice model need value high possible value called r squared value lie known coefficient determination case simple linear regression coefficient determination square correlation coefficient r x called r square correlation coefficient x ycovxyrootvarxvary r square ideally close sir explained theory behind interval estimate point estimate"
        ],
        [
         "1",
         "2025/01/22 10:58:35 AM GMT+5:30",
         "u02",
         "U02",
         "learnt analysing data in excel and different new terms of data analysis",
         null,
         null,
         "71",
         "12",
         "learnt analysing data excel different new term data analysis"
        ],
        [
         "2",
         "2025/01/22 11:07:54 AM GMT+5:30",
         "u03",
         "U03",
         "in todayâ€™s hands-on class, we worked with a dataset to perform regression analysis. using excel, we calculated key values like x_bar (average of x) and y_bar (average of y) manually using formulas. we explored the concept of error and its gaussian (normal) distribution, and understood the sum of squares of residuals (ssr), total sum of squares (sst), and explained sum of squares (sse). we discussed how well the variance is captured by the regression line and learned that, for simple linear regression, the coefficient of determination (r square) is the square of the correlation coefficient (r). finally, we used the data analysis toolpak in excel to quickly generate regression statistics for the dataset. additionally, we discussed sampling concepts, including calculating sample means from multiple samples, finding the mean of these sample means, and determining their variance and standard deviation. we understood that there is some relationship between the variance of sample means and the variance of the original data.",
         null,
         null,
         "1034",
         "159",
         "today handson class worked dataset perform regression analysis using excel calculated key value like xbar average x ybar average manually using formula explored concept error gaussian normal distribution understood sum square residual ssr total sum square sst explained sum square sse discussed well variance captured regression line learned simple linear regression coefficient determination r square square correlation coefficient r finally used data analysis toolpak excel quickly generate regression statistic dataset additionally discussed sampling concept including calculating sample mean multiple sample finding mean sample mean determining variance standard deviation understood relationship variance sample mean variance original data"
        ],
        [
         "3",
         "2025/01/22 11:10:12 AM GMT+5:30",
         "u04",
         "U04",
         "we first compared about the estimators being the statistics of the population. later we used excel to perform linear reg with the help of the formula provided in prev and class and performing the linear reg with the help of xlminer toolpack. later we talked about the ei errors in the regression. commenting on the error whether it is random or uniform. usually when the model explains the data variance successfully the errors should be close to a random. how should we comment on randomly distributed errors of regression? it should follow a gaussian distribution. then we can say our model knows all the parameters which can be the reason for the variation of the data. there were several results statistics like r^2 anova which given when we used xlminer. so like r^2 shows how much variation of the data does the particular model explains. we later moved on the proof of r^2 which is derived from sst,ssr,sse. r^2 was ssr/sst. r^2 is also called coef of determination. but why square? for the case of slr, the coeff of determination is the sq(correlation between x&y). since correlation coeff=> r. we call coeff of determination as r^2 (but only for slr(simple linear regression)). what is correlation coef. : as x changes wrt to its mean. how y changes wrt its mean. positive and negative correlation coeff shows how y changes wrt x (positively or negatively.) higher the value of r^2 higher explanation the model gives on the data variance. later at last we touched upon the standard error.",
         null,
         null,
         "1497",
         "257",
         "first compared estimator statistic population later used excel perform linear reg help formula provided prev class performing linear reg help xlminer toolpack later talked ei error regression commenting error whether random uniform usually model explains data variance successfully error close random comment randomly distributed error regression follow gaussian distribution say model know parameter reason variation data several result statistic like r anova given used xlminer like r show much variation data particular model explains later moved proof r derived sstssrsse r ssrsst r also called coef determination square case slr coeff determination sqcorrelation xy since correlation coeff r call coeff determination r slrsimple linear regression correlation coef x change wrt mean change wrt mean positive negative correlation coeff show change wrt x positively negatively higher value r higher explanation model give data variance later last touched upon standard error"
        ],
        [
         "4",
         "2025/01/22 11:25:40 AM GMT+5:30",
         "u05",
         "U05",
         "when predicting with a given sample size using linear regression (or any regression), there are limits. we can make predictions, but only up to a certain point , not far beyond the sample.\n\ntime series analysis: time series analysis helps to predict beyond the sample size. \n\nunderstanding f(x): in the equation ð‘“(ð‘¥)=ð‘¦ , the trend of the data is the signal, while the deviations are the noise. if there is a trend in the noise, it means your model has not captured the trend perfectly.\n\nhistogram: a histogram is simply a frequency chart that shows how often different values appear in a dataset.\n\na good model: a good model is one that explains most of the variation in the data.\n\nin simple linear regression (slr):\nsst=sse+ssr\nwhere:\nsse: sum of squares of errors, or the variation not explained by the regression line.\nssr: sum of squares of regression, or the total variation explained by the regression line.\n\nr-squared (râ²):\nrâ² = ssr/sst = 1 - sse/sst \nrâ² is the coefficient of determination, which measures how well the regression model explains the variation in the data.\n\nwhy râ² and not r? in simple linear regression (slr), the coefficient of determination (râ²) is exactly the same as the square of the correlation coefficient between ð‘¥ and ð‘¦ this is why we use râ²  rather than ð‘….\n\nwe also created an excel program for linear regression and learned about some of the major terms used to evaluate how well the model explains the variation in the data.",
         null,
         null,
         "1484",
         "254",
         "predicting given sample size using linear regression regression limit make prediction certain point far beyond sample time series analysis time series analysis help predict beyond sample size understanding fx equation trend data signal deviation noise trend noise mean model captured trend perfectly histogram histogram simply frequency chart show often different value appear dataset good model good model one explains variation data simple linear regression slr sstssessr sse sum square error variation explained regression line ssr sum square regression total variation explained regression line rsquared r r ssrsst ssesst r coefficient determination measure well regression model explains variation data r r simple linear regression slr coefficient determination r exactly square correlation coefficient use r rather also created excel program linear regression learned major term used evaluate well model explains variation data"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>RollNo</th>\n",
       "      <th>Session_Summary</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Total_Characters</th>\n",
       "      <th>Total_Words</th>\n",
       "      <th>PP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025/01/22 10:57:42 AM GMT+5:30</td>\n",
       "      <td>u01</td>\n",
       "      <td>U01</td>\n",
       "      <td>the class started with a hands on demo. sir up...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1551</td>\n",
       "      <td>253</td>\n",
       "      <td>class started hand demo sir uploaded file mood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025/01/22 10:58:35 AM GMT+5:30</td>\n",
       "      <td>u02</td>\n",
       "      <td>U02</td>\n",
       "      <td>learnt analysing data in excel and different n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "      <td>learnt analysing data excel different new term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025/01/22 11:07:54 AM GMT+5:30</td>\n",
       "      <td>u03</td>\n",
       "      <td>U03</td>\n",
       "      <td>in todayâ€™s hands-on class, we worked with a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1034</td>\n",
       "      <td>159</td>\n",
       "      <td>today handson class worked dataset perform reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025/01/22 11:10:12 AM GMT+5:30</td>\n",
       "      <td>u04</td>\n",
       "      <td>U04</td>\n",
       "      <td>we first compared about the estimators being t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497</td>\n",
       "      <td>257</td>\n",
       "      <td>first compared estimator statistic population ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025/01/22 11:25:40 AM GMT+5:30</td>\n",
       "      <td>u05</td>\n",
       "      <td>U05</td>\n",
       "      <td>when predicting with a given sample size using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1484</td>\n",
       "      <td>254</td>\n",
       "      <td>predicting given sample size using linear regr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp Username RollNo  \\\n",
       "0  2025/01/22 10:57:42 AM GMT+5:30      u01    U01   \n",
       "1  2025/01/22 10:58:35 AM GMT+5:30      u02    U02   \n",
       "2  2025/01/22 11:07:54 AM GMT+5:30      u03    U03   \n",
       "3  2025/01/22 11:10:12 AM GMT+5:30      u04    U04   \n",
       "4  2025/01/22 11:25:40 AM GMT+5:30      u05    U05   \n",
       "\n",
       "                                     Session_Summary  Questions Comments  \\\n",
       "0  the class started with a hands on demo. sir up...        NaN      NaN   \n",
       "1  learnt analysing data in excel and different n...        NaN      NaN   \n",
       "2  in todayâ€™s hands-on class, we worked with a ...        NaN      NaN   \n",
       "3  we first compared about the estimators being t...        NaN      NaN   \n",
       "4  when predicting with a given sample size using...        NaN      NaN   \n",
       "\n",
       "   Total_Characters  Total_Words  \\\n",
       "0              1551          253   \n",
       "1                71           12   \n",
       "2              1034          159   \n",
       "3              1497          257   \n",
       "4              1484          254   \n",
       "\n",
       "                                                  PP  \n",
       "0  class started hand demo sir uploaded file mood...  \n",
       "1  learnt analysing data excel different new term...  \n",
       "2  today handson class worked dataset perform reg...  \n",
       "3  first compared estimator statistic population ...  \n",
       "4  predicting given sample size using linear regr...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit_transform(df['PP']).toarray()\n",
    "    df['count_vector'] = list(vectors)\n",
    "    vocabulary = vectorizer.vocabulary_  # Get the vocabulary\n",
    "\n",
    "    return vectors, vocabulary\n",
    "\n",
    "# Word2Vec vectorization function\n",
    "def word2vec_vectorize(df):\n",
    "    tokenized = df['PP'].apply(lambda x: x.split())\n",
    "    model = Word2Vec(sentences=tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    def vectorize(text):\n",
    "        words = text.split()\n",
    "        return np.mean([model.wv[word] for word in words if word in model.wv] or [np.zeros(100)], axis=0)\n",
    "\n",
    "    df['word2vec'] = df['PP'].apply(vectorize)\n",
    "    return np.array(df['word2vec'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34feabf3-40d7-4483-ad2a-32885ecb8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorize(df, text_column='PP', max_features=1000):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(df[text_column]).toarray()\n",
    "    df['tfidf_vector'] = list(tfidf_vectors)\n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0826448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vectorizations\n",
    "count_vectors, vocab = count_vectorize(df_work)\n",
    "word2vec_vectors = word2vec_vectorize(df_work)\n",
    "tfidf_vectors = tfidf_vectorize(df_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f787b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Username",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RollNo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Session_Summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Questions",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Comments",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Total_Characters",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PP",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count_vector",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "word2vec",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tfidf_vector",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "93d2c51d-daf4-47e8-b31c-440dd40c0ea7",
       "rows": [
        [
         "0",
         "2025/01/22 10:57:42 AM GMT+5:30",
         "u01",
         "U01",
         "the class started with a hands on demo. sir uploaded a file on moodle. we then downloaded the file and applied simple linear regression on it. sir then explained about some features in excel and how to use them. sir then focused on errors in linear regression. then sir explained about histogram. outcome dependent on a large number of unknown causes(random), the distribution is gaussian normal distribution.\nwe need to understand what each number we get from analysis tells us. lower and upper 95% means that the real value of population lies with 95% confidence in the interval [lower 95,upper 95].\nwhat is a good model- one that explains most of the variations in the data. \nsst=sum of (yi-y_bar)â² (measure of total variation in given dataset)\nsst=sse+ssr\nssr- sum of square of regression line (total variation explained by the regression line)\nsse- variation not explained by the model, attributed to random errors. \nsst=ssr+sse\n1=(ssr/sst)+sse/sst\nssr/sst portion of total variation described by the regression model. for a nice model we need this value to be as high as possible. this value is called r squared value. lies in between 0 and 1.  this is known as coefficient of determination. for the case of simple linear regression, the coefficient of determination is same as square of the correlation coefficient r between x and y. so this is called 'r square'. correlation coefficient of x and y=cov(xy)/root(var(x)*var(y))\nr square should be ideally be close to 1. \nthen sir explained theory behind interval estimates and point estimates. ",
         null,
         null,
         "1551",
         "253",
         "class started hand demo sir uploaded file moodle downloaded file applied simple linear regression sir explained feature excel use sir focused error linear regression sir explained histogram outcome dependent large number unknown causesrandom distribution gaussian normal distribution need understand number get analysis tell u lower upper mean real value population lie confidence interval lower upper good model one explains variation data sstsum yiybar measure total variation given dataset sstssessr ssr sum square regression line total variation explained regression line sse variation explained model attributed random error sstssrsse ssrsstssesst ssrsst portion total variation described regression model nice model need value high possible value called r squared value lie known coefficient determination case simple linear regression coefficient determination square correlation coefficient r x called r square correlation coefficient x ycovxyrootvarxvary r square ideally close sir explained theory behind interval estimate point estimate",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 5 1 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 3 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 1 0 0 0 0 0 0\n 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 1 0 1 0 0 1 1 1\n 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 2 1 0 0 0\n 0 0 0 0 0 0 0 4 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n 0 1 0 0 0 0]",
         "[-9.8596076e-03  3.8282499e-02  4.6141213e-04  1.8042079e-03\n  1.7823273e-02 -8.4031224e-02  2.4463106e-02  1.0235916e-01\n -3.8112249e-02 -2.6500074e-02 -3.3631958e-02 -7.0363007e-02\n  9.9193342e-03  1.7697796e-02  1.9611845e-02 -2.6358224e-02\n  9.7975479e-03 -5.0550874e-02 -3.2687932e-03 -9.6165709e-02\n  3.0920580e-02  2.5498468e-02  1.9252919e-02 -2.2812249e-02\n -1.0838713e-02  1.6571671e-02 -4.6286643e-02 -2.7317706e-02\n -3.3590551e-02  1.6243888e-04  5.4744702e-02  1.0432001e-02\n  3.2210767e-02 -2.9655201e-02 -2.6723975e-02  6.0300447e-02\n -3.4390076e-04 -3.3453923e-02 -3.2798301e-02 -9.4663821e-02\n  4.0563056e-03 -3.9952219e-02 -3.1474914e-02  5.3761603e-04\n  4.3891892e-02 -1.5204668e-04 -3.7732240e-02 -1.7385455e-02\n  3.8724329e-02  2.0032451e-02  1.8899508e-02 -3.3062652e-02\n -9.5166289e-04 -1.3119498e-02 -2.0647902e-02  2.1207543e-02\n  1.0926923e-02  3.1357707e-05 -5.3577535e-02  1.2675912e-02\n  1.1813557e-02  4.7166348e-03  3.7921940e-03 -1.2941050e-03\n -5.1513478e-02  3.8718920e-02  2.3156255e-02  3.7282098e-02\n -6.9908619e-02  6.0336821e-02 -3.6316853e-02  1.3683338e-02\n  4.4315215e-02 -7.3149935e-03  6.6804849e-02  2.6440855e-02\n  2.5753574e-03 -1.0225189e-02 -4.1952763e-02  1.7623004e-02\n -2.6212648e-02 -6.9846348e-03 -6.2063791e-02  6.6010706e-02\n -2.7162412e-02 -1.4244351e-02  3.0799372e-02  5.4420192e-02\n  5.2979998e-02  4.3841284e-03  7.7765286e-02  3.6587656e-02\n -1.7342871e-03  1.6835410e-02  8.5580364e-02  4.8306178e-02\n  2.3699306e-02 -3.9805502e-02  2.0008666e-02 -7.4104019e-03]",
         "[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.03488096 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.09105822\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.08105591 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.10815726 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.136909   0.\n 0.         0.         0.         0.         0.0584522  0.\n 0.10815726 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.05015186 0.06395686 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.13952385 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.04790771 0.         0.         0.         0.         0.\n 0.         0.         0.07510091 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.02640056 0.0584522  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.09815496 0.\n 0.         0.         0.         0.05686012 0.         0.\n 0.         0.         0.         0.         0.         0.10815726\n 0.         0.         0.         0.08976653 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.06494469\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.10815726 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.05750348\n 0.         0.         0.         0.15450643 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.02731432 0.         0.         0.\n 0.         0.         0.         0.25677727 0.06395686 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.08105591 0.14210722 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.08555355 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.05262079\n 0.         0.         0.         0.         0.         0.\n 0.         0.0684545  0.         0.05536459 0.         0.\n 0.         0.04305512 0.         0.         0.         0.\n 0.         0.08105591 0.         0.         0.         0.\n 0.         0.         0.         0.         0.09105822 0.\n 0.         0.         0.         0.03425641 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.10815726 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.10030373 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.08105591 0.\n 0.05686012 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.16211183\n 0.         0.         0.         0.         0.08108854 0.09242597\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.15450643 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.03425641 0.         0.         0.         0.\n 0.05686012 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.12988939 0.         0.\n 0.         0.09815496 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.1711071  0.         0.         0.         0.\n 0.         0.10815726 0.         0.         0.         0.03900197\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.12030833 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.06198231 0.         0.         0.         0.         0.\n 0.06610332 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.06015417 0.         0.04585079 0.09815496\n 0.         0.         0.         0.08555355 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.04305512 0.         0.         0.         0.         0.\n 0.         0.09105822 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.16111927 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.08790452 0.         0.         0.         0.40527956\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.15306215\n 0.         0.07105361 0.         0.04054427 0.         0.\n 0.04790771 0.07105361 0.10815726 0.         0.09815496 0.08555355\n 0.10815726 0.         0.06395686 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.05015186 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.09815496 0.         0.         0.         0.         0.\n 0.         0.         0.09815496 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.15045559\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.07395917\n 0.         0.         0.         0.         0.         0.\n 0.         0.06610332 0.         0.         0.08105591 0.\n 0.         0.1711071  0.06395686 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.14742205 0.         0.         0.         0.\n 0.         0.21527558 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.10815726 0.         0.         0.         0.\n 0.         0.         0.         0.10815726 0.         0.\n 0.         0.        ]"
        ],
        [
         "1",
         "2025/01/22 10:58:35 AM GMT+5:30",
         "u02",
         "U02",
         "learnt analysing data in excel and different new terms of data analysis",
         null,
         null,
         "71",
         "12",
         "learnt analysing data excel different new term data analysis",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]",
         "[-8.50011501e-03  4.28209715e-02  3.43192904e-03  3.79022676e-03\n  1.92691106e-02 -9.94346216e-02  2.42945179e-02  1.16467036e-01\n -4.55214232e-02 -3.28868665e-02 -4.06804197e-02 -8.30461681e-02\n  1.11995237e-02  2.37787943e-02  2.65023410e-02 -2.87828110e-02\n  9.92091652e-03 -5.49626835e-02 -1.30343577e-03 -1.05765253e-01\n  3.72450128e-02  2.49766484e-02  2.17082184e-02 -2.96417512e-02\n -1.00027695e-02  1.93956550e-02 -5.51323891e-02 -3.01884487e-02\n -3.84620391e-02  3.33090953e-04  6.55189604e-02  1.16432188e-02\n  3.90821137e-02 -3.22638303e-02 -2.93946061e-02  6.82832375e-02\n  3.59213143e-03 -3.57593410e-02 -3.51039320e-02 -1.05205461e-01\n  7.29513681e-03 -4.65914309e-02 -3.68678048e-02  1.06934540e-03\n  4.86045182e-02  2.36349343e-03 -4.35211696e-02 -1.69403851e-02\n  4.22013514e-02  2.18510311e-02  2.21765414e-02 -4.05674055e-02\n -4.80832625e-03 -1.54044433e-02 -2.13057697e-02  2.69155316e-02\n  1.35436999e-02  1.87412754e-03 -5.95403984e-02  1.30294757e-02\n  1.16630830e-02  6.45377859e-03  2.94979638e-03 -1.97661320e-05\n -6.14079982e-02  4.45208363e-02  2.67571881e-02  4.10028547e-02\n -7.60662705e-02  7.09885284e-02 -4.04812023e-02  1.35269575e-02\n  5.05735725e-02 -2.80023227e-03  7.72751719e-02  3.18185687e-02\n  8.75572267e-04 -1.48809506e-02 -4.80314232e-02  2.06878316e-02\n -2.95836497e-02 -7.02491403e-03 -6.65710717e-02  7.47219026e-02\n -2.90554035e-02 -1.35859903e-02  3.44251655e-02  6.06164597e-02\n  5.99626563e-02  2.05964176e-03  9.07888561e-02  3.96281034e-02\n -1.78891851e-03  1.78300105e-02  9.15007666e-02  5.60930520e-02\n  2.72089001e-02 -4.68539111e-02  2.22271010e-02 -9.70018003e-03]",
         "[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.531905   0.1890211  0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.28613104 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.38504188 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.14801722 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.29238157 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.4934475  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.31675442 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]"
        ],
        [
         "2",
         "2025/01/22 11:07:54 AM GMT+5:30",
         "u03",
         "U03",
         "in todayâ€™s hands-on class, we worked with a dataset to perform regression analysis. using excel, we calculated key values like x_bar (average of x) and y_bar (average of y) manually using formulas. we explored the concept of error and its gaussian (normal) distribution, and understood the sum of squares of residuals (ssr), total sum of squares (sst), and explained sum of squares (sse). we discussed how well the variance is captured by the regression line and learned that, for simple linear regression, the coefficient of determination (r square) is the square of the correlation coefficient (r). finally, we used the data analysis toolpak in excel to quickly generate regression statistics for the dataset. additionally, we discussed sampling concepts, including calculating sample means from multiple samples, finding the mean of these sample means, and determining their variance and standard deviation. we understood that there is some relationship between the variance of sample means and the variance of the original data.",
         null,
         null,
         "1034",
         "159",
         "today handson class worked dataset perform regression analysis using excel calculated key value like xbar average x ybar average manually using formula explored concept error gaussian normal distribution understood sum square residual ssr total sum square sst explained sum square sse discussed well variance captured regression line learned simple linear regression coefficient determination r square square correlation coefficient r finally used data analysis toolpak excel quickly generate regression statistic dataset additionally discussed sampling concept including calculating sample mean multiple sample finding mean sample mean determining variance standard deviation understood relationship variance sample mean variance original data",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 2 0 0 0\n 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 1 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 1 0 0 1 0 0\n 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2\n 0 0 0 0 0 0 0 1 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]",
         "[-9.96951386e-03  3.97085585e-02 -2.92617013e-04  2.32343446e-03\n  1.80631485e-02 -9.07136276e-02  2.66454164e-02  1.10089116e-01\n -4.19996269e-02 -2.85150483e-02 -3.68280150e-02 -7.66271725e-02\n  1.07278721e-02  1.82646848e-02  2.21689921e-02 -2.94889044e-02\n  1.16136596e-02 -5.53096123e-02 -2.04163324e-03 -1.04217120e-01\n  3.36396284e-02  2.72550881e-02  2.11629178e-02 -2.42072344e-02\n -1.18493624e-02  1.79143529e-02 -4.99196015e-02 -3.02809682e-02\n -3.58409546e-02  4.14957292e-04  5.89525364e-02  1.28333271e-02\n  3.36378589e-02 -3.25878970e-02 -2.79021244e-02  6.46070465e-02\n  2.55374907e-04 -3.69718634e-02 -3.44619080e-02 -1.03029318e-01\n  3.91427940e-03 -4.34643030e-02 -3.45043503e-02  1.88501284e-03\n  4.72763777e-02 -6.08208298e-04 -4.08913977e-02 -1.76340844e-02\n  4.22957279e-02  2.26402581e-02  2.07897071e-02 -3.53047960e-02\n -6.80182537e-04 -1.39614670e-02 -2.15021241e-02  2.34910492e-02\n  1.24933766e-02  6.45907130e-04 -5.88412471e-02  1.32132759e-02\n  1.19837560e-02  6.06113905e-03  3.13976035e-03 -6.67116183e-05\n -5.67170940e-02  4.20106351e-02  2.33061686e-02  4.04938422e-02\n -7.53516033e-02  6.49899393e-02 -3.85472625e-02  1.40573708e-02\n  4.80937548e-02 -6.64141821e-03  7.13433549e-02  2.85836067e-02\n  3.59353865e-03 -1.09501705e-02 -4.40868288e-02  1.86073165e-02\n -2.84287483e-02 -7.13125011e-03 -6.66717142e-02  7.05075935e-02\n -2.96152160e-02 -1.48648918e-02  3.25593986e-02  6.00538291e-02\n  5.73395975e-02  4.54076054e-03  8.55988786e-02  3.93442959e-02\n -1.96755654e-03  1.87877994e-02  9.18907523e-02  5.24236560e-02\n  2.59192716e-02 -4.35653068e-02  2.19599623e-02 -9.43694171e-03]",
         "[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.12154249\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.09910797 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.20188602 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.07124874 0.10975054 0.         0.         0.         0.\n 0.         0.         0.         0.09725056 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.07124874 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.09910797 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.18782067 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.05334642 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.07501244 0.16608139 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06376385 0.         0.\n 0.         0.         0.         0.15365468 0.         0.08545861\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.15330216 0.         0.         0.\n 0.         0.         0.         0.         0.         0.04613216\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.04084644\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.07760872 0.         0.         0.\n 0.         0.         0.         0.07295863 0.         0.\n 0.10507082 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.12936275 0.\n 0.12154249 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.07665108\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.07475625\n 0.         0.         0.         0.         0.         0.15365468\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.13944481 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.12154249 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.10507082\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.08545861 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.06806056 0.         0.         0.         0.05759961 0.04376862\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.12154249 0.         0.         0.         0.         0.\n 0.         0.1946668  0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.09391034\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.05540854\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.10094301\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.10975054 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.15365468\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.15259712 0.         0.         0.         0.\n 0.08545861 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.11515288 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.19821593 0.08545861 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.06244121 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.27181157\n 0.         0.         0.         0.05759961 0.         0.\n 0.06806056 0.         0.         0.07295863 0.         0.\n 0.         0.07295863 0.         0.         0.         0.\n 0.09725056 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.21374622 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.06244121 0.\n 0.         0.         0.         0.         0.10975054 0.07124874\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.24308499 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06513838 0.         0.11297391\n 0.         0.         0.         0.         0.         0.\n 0.         0.05235915 0.         0.         0.         0.\n 0.33216278 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.08805578 0.         0.         0.         0.\n 0.         0.         0.         0.         0.12154249 0.\n 0.         0.         0.         0.10507082 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.10975054 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]"
        ],
        [
         "3",
         "2025/01/22 11:10:12 AM GMT+5:30",
         "u04",
         "U04",
         "we first compared about the estimators being the statistics of the population. later we used excel to perform linear reg with the help of the formula provided in prev and class and performing the linear reg with the help of xlminer toolpack. later we talked about the ei errors in the regression. commenting on the error whether it is random or uniform. usually when the model explains the data variance successfully the errors should be close to a random. how should we comment on randomly distributed errors of regression? it should follow a gaussian distribution. then we can say our model knows all the parameters which can be the reason for the variation of the data. there were several results statistics like r^2 anova which given when we used xlminer. so like r^2 shows how much variation of the data does the particular model explains. we later moved on the proof of r^2 which is derived from sst,ssr,sse. r^2 was ssr/sst. r^2 is also called coef of determination. but why square? for the case of slr, the coeff of determination is the sq(correlation between x&y). since correlation coeff=> r. we call coeff of determination as r^2 (but only for slr(simple linear regression)). what is correlation coef. : as x changes wrt to its mean. how y changes wrt its mean. positive and negative correlation coeff shows how y changes wrt x (positively or negatively.) higher the value of r^2 higher explanation the model gives on the data variance. later at last we touched upon the standard error.",
         null,
         null,
         "1497",
         "257",
         "first compared estimator statistic population later used excel perform linear reg help formula provided prev class performing linear reg help xlminer toolpack later talked ei error regression commenting error whether random uniform usually model explains data variance successfully error close random comment randomly distributed error regression follow gaussian distribution say model know parameter reason variation data several result statistic like r anova given used xlminer like r show much variation data particular model explains later moved proof r derived sstssrsse r ssrsst r also called coef determination square case slr coeff determination sqcorrelation xy since correlation coeff r call coeff determination r slrsimple linear regression correlation coef x change wrt mean change wrt mean positive negative correlation coeff show change wrt x positively negatively higher value r higher explanation model give data variance later last touched upon standard error",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0\n 0 0 3 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 2 4 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 5 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0\n 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 4\n 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 1 0 1 0 0 0\n 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 2 0 1 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0\n 0 0 1 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 2 0 0\n 1 0 0 0 0 0 0 1 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]",
         "[-0.00901402  0.03320912  0.00089525  0.00200275  0.01565292 -0.07226961\n  0.0212032   0.08732168 -0.03390909 -0.02276653 -0.02919507 -0.06086631\n  0.00810643  0.01486015  0.01869996 -0.02365839  0.00892698 -0.04386843\n -0.00265344 -0.08273117  0.02742051  0.0215721   0.01689767 -0.01974841\n -0.00891789  0.01554298 -0.03988141 -0.02396777 -0.0294275  -0.00022415\n  0.04816936  0.01012274  0.02735168 -0.02589558 -0.02222781  0.05228778\n -0.00020195 -0.02945619 -0.02812604 -0.08254383  0.00334976 -0.03518322\n -0.02720924  0.00202488  0.03834649  0.00110699 -0.03322871 -0.01455493\n  0.03364567  0.01672008  0.01679631 -0.02930811 -0.00087803 -0.01173662\n -0.01942807  0.01837896  0.00908527  0.00118852 -0.0470886   0.010869\n  0.01042683  0.00446665  0.00226417 -0.00070779 -0.04407215  0.03366603\n  0.01938326  0.03354304 -0.05989114  0.05228645 -0.03231223  0.01069597\n  0.03713397 -0.00514334  0.05758266  0.02407687  0.0029768  -0.00819439\n -0.03615028  0.0159922  -0.02299608 -0.00700523 -0.05339168  0.05664159\n -0.02381379 -0.01216938  0.02722015  0.04766696  0.04649791  0.00226548\n  0.06766159  0.03167634 -0.00210297  0.01437024  0.07369568  0.04278419\n  0.02158199 -0.03526393  0.01702693 -0.00607707]",
         "[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.03352589 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.07236855 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.08763496 0.06111772 0.\n 0.         0.         0.         0.         0.05218744 0.\n 0.         0.         0.         0.         0.         0.24389648\n 0.         0.         0.         0.         0.         0.\n 0.         0.04477671 0.05710213 0.         0.         0.\n 0.         0.         0.         0.         0.19313049 0.38626097\n 0.         0.         0.         0.         0.         0.09656524\n 0.09656524 0.         0.08129883 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.10057767 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.09428406 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.05218744 0.         0.\n 0.         0.         0.         0.12021837 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06111772 0.         0.02899204\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.08129883 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.12835101\n 0.         0.         0.         0.         0.         0.\n 0.         0.09656524 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.02438684 0.         0.         0.\n 0.         0.         0.         0.         0.11420426 0.09656524\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06343826 0.         0.\n 0.         0.         0.         0.         0.05901854 0.\n 0.         0.         0.         0.         0.         0.04817185\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.04698103\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.05370699 0.04943075 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.10741399 0.         0.         0.         0.14473709\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.06343826 0.         0.         0.\n 0.         0.         0.         0.08129883 0.         0.30553655\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.08554616 0.         0.         0.         0.         0.08251999\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.06116978 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.11596817 0.         0.\n 0.         0.         0.08129883 0.         0.06343826 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.07236855 0.08763496 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.05076599 0.07638414 0.\n 0.         0.         0.         0.06897341 0.         0.\n 0.06897341 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.04093662 0.\n 0.07236855 0.09656524 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.09656524 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.09656524 0.         0.         0.         0.\n 0.06343826 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.07688116 0.         0.07236855 0.         0.         0.\n 0.         0.         0.         0.09656524 0.         0.\n 0.         0.         0.         0.17526992 0.         0.\n 0.         0.07192546 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.05710213 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.07638414 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.09656524 0.         0.         0.\n 0.         0.13794682 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.08763496 0.         0.\n 0.         0.         0.05370699 0.09656524 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.09656524 0.         0.         0.03416433\n 0.         0.         0.         0.         0.         0.\n 0.         0.06343826 0.         0.         0.         0.07638414\n 0.         0.04585131 0.         0.         0.         0.\n 0.12223544 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.09656524\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.07638414 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.07236855 0.         0.\n 0.         0.09656524 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.05710213 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.07638414 0.         0.         0.08187324 0.         0.\n 0.09656524 0.         0.         0.         0.         0.\n 0.         0.03290543 0.         0.         0.         0.\n 0.10437488 0.07688116 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.07236855 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.26290489 0.         0.         0.         0.\n 0.         0.19313049 0.         0.         0.         0.\n 0.07638414 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]"
        ],
        [
         "4",
         "2025/01/22 11:25:40 AM GMT+5:30",
         "u05",
         "U05",
         "when predicting with a given sample size using linear regression (or any regression), there are limits. we can make predictions, but only up to a certain point , not far beyond the sample.\n\ntime series analysis: time series analysis helps to predict beyond the sample size. \n\nunderstanding f(x): in the equation ð‘“(ð‘¥)=ð‘¦ , the trend of the data is the signal, while the deviations are the noise. if there is a trend in the noise, it means your model has not captured the trend perfectly.\n\nhistogram: a histogram is simply a frequency chart that shows how often different values appear in a dataset.\n\na good model: a good model is one that explains most of the variation in the data.\n\nin simple linear regression (slr):\nsst=sse+ssr\nwhere:\nsse: sum of squares of errors, or the variation not explained by the regression line.\nssr: sum of squares of regression, or the total variation explained by the regression line.\n\nr-squared (râ²):\nrâ² = ssr/sst = 1 - sse/sst \nrâ² is the coefficient of determination, which measures how well the regression model explains the variation in the data.\n\nwhy râ² and not r? in simple linear regression (slr), the coefficient of determination (râ²) is exactly the same as the square of the correlation coefficient between ð‘¥ and ð‘¦ this is why we use râ²  rather than ð‘….\n\nwe also created an excel program for linear regression and learned about some of the major terms used to evaluate how well the model explains the variation in the data.",
         null,
         null,
         "1484",
         "254",
         "predicting given sample size using linear regression regression limit make prediction certain point far beyond sample time series analysis time series analysis help predict beyond sample size understanding fx equation trend data signal deviation noise trend noise mean model captured trend perfectly histogram histogram simply frequency chart show often different value appear dataset good model good model one explains variation data simple linear regression slr sstssessr sse sum square error variation explained regression line ssr sum square regression total variation explained regression line rsquared r r ssrsst ssesst r coefficient determination measure well regression model explains variation data r r simple linear regression slr coefficient determination r exactly square correlation coefficient use r rather also created excel program linear regression learned major term used evaluate well model explains variation data",
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0\n 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 4 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 2 3 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 2\n 1 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 1 0 1 1 1 0\n 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1\n 0 0 0 0 0 0 0 1 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0]",
         "[-1.14064710e-02  4.39063758e-02  1.25914952e-03  3.36362584e-03\n  1.96919236e-02 -9.52325985e-02  2.74057370e-02  1.15811594e-01\n -4.40349877e-02 -3.01495027e-02 -3.75830680e-02 -8.01851600e-02\n  1.06224325e-02  2.06129439e-02  2.32226551e-02 -2.99347527e-02\n  1.10620130e-02 -5.71467876e-02 -3.63230333e-03 -1.08067028e-01\n  3.63263935e-02  2.88229343e-02  2.21433714e-02 -2.46534292e-02\n -1.12949079e-02  1.89868510e-02 -5.11554368e-02 -3.09984796e-02\n -3.75567041e-02 -1.32013141e-04  6.22103848e-02  1.26401428e-02\n  3.55419703e-02 -3.32683772e-02 -2.94655133e-02  6.72986284e-02\n  8.10176600e-04 -3.83844525e-02 -3.63930948e-02 -1.07299045e-01\n  5.02680242e-03 -4.53169607e-02 -3.57130803e-02  7.41758267e-04\n  4.92871627e-02  5.12120314e-05 -4.34657075e-02 -1.83911007e-02\n  4.40390743e-02  2.30081007e-02  2.12357845e-02 -3.70149538e-02\n -1.55886973e-03 -1.50798121e-02 -2.30160281e-02  2.37651970e-02\n  1.33977495e-02 -2.29988964e-05 -6.13194332e-02  1.37972562e-02\n  1.27197737e-02  6.23210752e-03  3.82200954e-03 -2.35721469e-03\n -5.93932867e-02  4.29786146e-02  2.53362563e-02  4.37521860e-02\n -7.83732459e-02  6.80665374e-02 -4.11338098e-02  1.41772451e-02\n  4.90693673e-02 -7.30330637e-03  7.59840012e-02  2.93884985e-02\n  3.25238914e-03 -1.14012836e-02 -4.74833250e-02  2.08629556e-02\n -2.92815249e-02 -8.67214520e-03 -6.86632320e-02  7.46833757e-02\n -3.04742809e-02 -1.57070719e-02  3.47982831e-02  6.21952489e-02\n  6.04423024e-02  4.02158173e-03  8.84388760e-02  4.12779525e-02\n -1.80662738e-03  1.89694818e-02  9.66448486e-02  5.57079688e-02\n  2.76479162e-02 -4.60806936e-02  2.24531107e-02 -8.22445750e-03]",
         "[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.04344021 0.         0.         0.         0.\n 0.         0.08070402 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.10534061 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.17874062 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.07919153 0.         0.\n 0.         0.         0.         0.         0.09897255 0.\n 0.         0.         0.09897255 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.12105603 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.04344021 0.         0.         0.\n 0.         0.         0.07647157 0.         0.         0.\n 0.         0.         0.         0.12216586 0.06762037 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.10384633 0.         0.\n 0.         0.         0.         0.         0.         0.06958929\n 0.         0.         0.0821983  0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.08555959 0.         0.03326142\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.09897255 0.         0.\n 0.         0.         0.         0.         0.09376946 0.\n 0.         0.         0.03159855 0.         0.         0.\n 0.         0.         0.         0.11882102 0.22196532 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.11355048 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.07647157\n 0.         0.         0.         0.         0.12512163 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06404847 0.         0.\n 0.         0.09961654 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.06958929 0.         0.         0.         0.\n 0.         0.         0.         0.07925898 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06958929 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.05542199 0.         0.         0.0938072  0.14256387\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.12512163 0.08937031 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.03962949 0.         0.         0.         0.\n 0.06577857 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.18782802 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.21068122 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.12512163\n 0.07170418 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.09897255 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.06958929 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.09897255 0.         0.         0.09897255 0.07170418\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.12512163\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.08937031\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.27958601 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.09897255 0.\n 0.12105603 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.18753891\n 0.         0.         0.         0.         0.         0.\n 0.         0.08937031 0.         0.         0.         0.\n 0.         0.         0.         0.12512163 0.         0.\n 0.         0.10169226 0.11355048 0.         0.         0.\n 0.12483457 0.         0.13917858 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.13280235\n 0.         0.         0.         0.0469036  0.         0.08555959\n 0.05542199 0.0821983  0.         0.         0.11355048 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.11603627 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.06762037 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.17111919 0.         0.         0.\n 0.         0.         0.         0.         0.         0.05801814\n 0.         0.         0.         0.         0.         0.\n 0.         0.2294147  0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.09897255 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.07398844 0.05304244 0.         0.04599756\n 0.         0.         0.         0.         0.         0.\n 0.         0.04263627 0.         0.         0.         0.\n 0.         0.24904136 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.14340836 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>RollNo</th>\n",
       "      <th>Session_Summary</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Total_Characters</th>\n",
       "      <th>Total_Words</th>\n",
       "      <th>PP</th>\n",
       "      <th>count_vector</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>tfidf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025/01/22 10:57:42 AM GMT+5:30</td>\n",
       "      <td>u01</td>\n",
       "      <td>U01</td>\n",
       "      <td>the class started with a hands on demo. sir up...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1551</td>\n",
       "      <td>253</td>\n",
       "      <td>class started hand demo sir uploaded file mood...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.009859608, 0.0382825, 0.00046141213, 0.001...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025/01/22 10:58:35 AM GMT+5:30</td>\n",
       "      <td>u02</td>\n",
       "      <td>U02</td>\n",
       "      <td>learnt analysing data in excel and different n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "      <td>learnt analysing data excel different new term...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.008500115, 0.04282097, 0.003431929, 0.0037...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025/01/22 11:07:54 AM GMT+5:30</td>\n",
       "      <td>u03</td>\n",
       "      <td>U03</td>\n",
       "      <td>in todayâ€™s hands-on class, we worked with a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1034</td>\n",
       "      <td>159</td>\n",
       "      <td>today handson class worked dataset perform reg...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.009969514, 0.03970856, -0.000292617, 0.002...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025/01/22 11:10:12 AM GMT+5:30</td>\n",
       "      <td>u04</td>\n",
       "      <td>U04</td>\n",
       "      <td>we first compared about the estimators being t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497</td>\n",
       "      <td>257</td>\n",
       "      <td>first compared estimator statistic population ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.009014022, 0.03320912, 0.0008952469, 0.002...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025/01/22 11:25:40 AM GMT+5:30</td>\n",
       "      <td>u05</td>\n",
       "      <td>U05</td>\n",
       "      <td>when predicting with a given sample size using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1484</td>\n",
       "      <td>254</td>\n",
       "      <td>predicting given sample size using linear regr...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.011406471, 0.043906376, 0.0012591495, 0.00...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp Username RollNo  \\\n",
       "0  2025/01/22 10:57:42 AM GMT+5:30      u01    U01   \n",
       "1  2025/01/22 10:58:35 AM GMT+5:30      u02    U02   \n",
       "2  2025/01/22 11:07:54 AM GMT+5:30      u03    U03   \n",
       "3  2025/01/22 11:10:12 AM GMT+5:30      u04    U04   \n",
       "4  2025/01/22 11:25:40 AM GMT+5:30      u05    U05   \n",
       "\n",
       "                                     Session_Summary  Questions Comments  \\\n",
       "0  the class started with a hands on demo. sir up...        NaN      NaN   \n",
       "1  learnt analysing data in excel and different n...        NaN      NaN   \n",
       "2  in todayâ€™s hands-on class, we worked with a ...        NaN      NaN   \n",
       "3  we first compared about the estimators being t...        NaN      NaN   \n",
       "4  when predicting with a given sample size using...        NaN      NaN   \n",
       "\n",
       "   Total_Characters  Total_Words  \\\n",
       "0              1551          253   \n",
       "1                71           12   \n",
       "2              1034          159   \n",
       "3              1497          257   \n",
       "4              1484          254   \n",
       "\n",
       "                                                  PP  \\\n",
       "0  class started hand demo sir uploaded file mood...   \n",
       "1  learnt analysing data excel different new term...   \n",
       "2  today handson class worked dataset perform reg...   \n",
       "3  first compared estimator statistic population ...   \n",
       "4  predicting given sample size using linear regr...   \n",
       "\n",
       "                                        count_vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            word2vec  \\\n",
       "0  [-0.009859608, 0.0382825, 0.00046141213, 0.001...   \n",
       "1  [-0.008500115, 0.04282097, 0.003431929, 0.0037...   \n",
       "2  [-0.009969514, 0.03970856, -0.000292617, 0.002...   \n",
       "3  [-0.009014022, 0.03320912, 0.0008952469, 0.002...   \n",
       "4  [-0.011406471, 0.043906376, 0.0012591495, 0.00...   \n",
       "\n",
       "                                        tfidf_vector  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f59c236f-49fc-4a17-8c2e-fe15d50cefa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to vocab.csv\n"
     ]
    }
   ],
   "source": [
    "vocabulary_list = vocab #sorted(list(vocab.keys()))\n",
    "# Save the vocabulary to a text file\n",
    "try:\n",
    "    filename = \"vocab.csv\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for word in vocabulary_list:\n",
    "            f.write(word + \",\" + str(vocab[word]) +'\\n')  # Write each word on a new line\n",
    "    print(f\"Vocabulary saved to {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving vocabulary: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66825a-5765-44c7-8887-a2508e5cf31f",
   "metadata": {},
   "source": [
    "### word2vec Playground!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b507d63-2db8-4b09-a448-873b94f43ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# List available models\n",
    "info = api.info()\n",
    "print(info['models'].keys())\n",
    "\n",
    "# Load a specific model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302833d9-6168-4b04-acdc-5c6b7723a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model\n",
    "w1 = wv['bicycle']\n",
    "w2  = wv['pedals']\n",
    "w3 = wv['engine']\n",
    "\n",
    "#print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdd1c0-780b-44e5-b13e-6547ada01309",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = w1 - w2 + w3\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87e0f4-ed40-4f25-81a5-7b2aa4b4ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 5 most similar words\n",
    "similar_words = wv.most_similar([result], topn=5)\n",
    "\n",
    "# Print the results\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
